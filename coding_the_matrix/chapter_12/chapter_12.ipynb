{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 12\n",
    "# The Eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.12 Lab: Pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12.12.1:** Write a procedure `find_num_links` with the following spec:\n",
    "* _input:_ A square matrix `L` representing a link structure.\n",
    "* _output:_ A vector `num_links` whose label set is the column-label set of `L`, such that, for each column-label `c`, entry `c` of `num_links` is the number of nonzero entries in column `c` of `L`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from vec import Vec\n",
    "\n",
    "def find_num_links(L):\n",
    "    ones = Vec(L.D[0], {k: 1 for k in L.D[0]})\n",
    "    return ones * L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12.12.2:** Write a procedure `make_Markov` with the following spec:\n",
    "* _input:_ A square matrix `L` representing a link structure as described above.\n",
    "* _ouptut:_ This procedure does not produce new output, but instead _mutates_ L so that it plays the role of $A_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Markov(L):\n",
    "    num_links = find_num_links(L)\n",
    "    for k in L.f.keys():\n",
    "        if k[0] == k[1] and num_links[k[1]] == 0:\n",
    "            L[k] = 1\n",
    "        else:\n",
    "            L[k] /= num_links[k[1]]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       1 2 3 4 5 6\n",
      "     -------------\n",
      " 1  |  1 0 0 1 0 0\n",
      " 2  |  0 0 1 1 1 1\n",
      " 3  |  0 1 0 0 0 0\n",
      " 4  |  0 0 0 0 1 0\n",
      " 5  |  0 0 0 0 0 1\n",
      " 6  |  0 0 0 0 1 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pagerank_test import small_links\n",
    "\n",
    "print(small_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 2 3 4 5 6\n",
      "------------\n",
      " 1 1 1 2 3 2\n"
     ]
    }
   ],
   "source": [
    "print(find_num_links(small_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       1 2 3   4     5   6\n",
      "     ---------------------\n",
      " 1  |  1 0 0 0.5     0   0\n",
      " 2  |  0 0 1 0.5 0.333 0.5\n",
      " 3  |  0 1 0   0     0   0\n",
      " 4  |  0 0 0   0 0.333   0\n",
      " 5  |  0 0 0   0     0 0.5\n",
      " 6  |  0 0 0   0 0.333   0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_Markov(small_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12.12.3:** Write a procedure `power_method` with the following spec:\n",
    "* _input:_\n",
    "  - the matrix $A_1$, and\n",
    "  - the desired number of iterations of the power method.\n",
    "* _output:_ an approximation to the stationary distribution, or at least a scalar multiple of the stationary distribution\n",
    "\n",
    "Your initial vector can be pretty much anything nonzero. We recommend using an all-ones vector.\n",
    "\n",
    "In order to see how well the method converges, at each iteration print the ratio\n",
    "\n",
    "  (norm of $v$ before the iteration)/(norm of $v$ after the iteration)\n",
    "\n",
    "As the approximation for the eigenvector with eigenvalue 1 gets better , this ratio should get closer to 1.\n",
    "\n",
    "Test your code using the matrix $A_1$ you obtained for the Thimble-Wide Web. The module `pagerank_test` defines `A2` to allow you to explicitly test whether the vector you get is an approximate eigenvector of $A$.\n",
    "\n",
    "You should obtain as an eigenvector a scalar multiple of the following vector:\n",
    "`{1: 0.5222, 2: 0.6182, 3: 0.5738, 4: 0.0705, 5: 0.783, 6: 0.0705}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(A1, num_iterations):\n",
    "    x = Vec(A1.D[0], {k: 1 for k in A1.D[0]})\n",
    "    for i in range(num_iterations):\n",
    "        x = A1 * x\n",
    "        norm = (x * x) ** 0.5\n",
    "        x = x / norm\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     1     2     3      4      5      6\n",
      "---------------------------------------\n",
      " 0.522 0.618 0.574 0.0705 0.0783 0.0705\n"
     ]
    }
   ],
   "source": [
    "from pagerank_test import A2\n",
    "print(power_method(small_links * 0.85 + A2 * 0.15, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12.12.5:** Write a procedure `wikigoogle` with the following spec:\n",
    "* _input:_ \n",
    "  - A single word `w`.\n",
    "  - The number `k` of desired results.\n",
    "  - The pagerank eigenvector `p`\n",
    "* _output:_ a list of the names of the `k` highest-pagerank wikipedia articles containing that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word meta-index\n",
      "Reading titles\n",
      "Reading link structure\n",
      "..................................................................................\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from pagerank import read_data, find_word\n",
    "\n",
    "links = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikigoogle(w, k, p):\n",
    "    related = find_word(w)\n",
    "    related.sort(key=lambda x: p[x], reverse=True)\n",
    "    return related[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12.12.6:** Use `power_method` to compute the pagerank eigenvector for the wikipedia corpus and try some queries to see the titles of the top few pages: \"jordan\", \"obama\", \"tiger\" and of course \"matrix\". What do you get for your top few articles? Can you explain why? Are the top ranked results more relevant or important in some sense than, say, the first few articles returned by `find_word` without ranking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mat import Mat\n",
    "\n",
    "A1 = make_Markov(links)\n",
    "A2 = Mat(A1.D, {k:1/len(A1.D[0]) for k in A1.f.keys()})\n",
    "A = 0.85*A1 + 0.15*A2\n",
    "\n",
    "p = power_method(A, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "jordan_results = wikigoogle('jordan', 5, p)\n",
    "obama_results  = wikigoogle('obama', 5, p)\n",
    "tiger_results  = wikigoogle('tiger', 5, p)\n",
    "matrix_results = wikigoogle('matrix', 5, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2007', '2006', '2005', 'paris', 'israel']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jordan_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united states',\n",
       " 'president of the united states',\n",
       " 'chicago',\n",
       " 'democratic party (united states)',\n",
       " 'illinois']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['france', 'england', '2006', 'india', 'sweden']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiger_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1999', 'internet', 'religion', '1965', 'sydney']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alabama', 'altruism', 'asphalt', 'arabic language', 'attila the hun']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_word('jordan')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['august 27', 'austin, texas', 'african american', 'august 4', 'the bronx']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_word('obama')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are indeed much better than the simple `find_word` query.  This makes sense since the eigenvector-weighted results are informed by the stationary distribution of a model involving actual links between pages rather than simply alphabetized documents with occurrences of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12.12.7:** Write a version of the power method that finds an approximation to the stationary distribution of a Markov chain that is biased. The procedure should be called `power_method_biased`. It resembles `power_method` but takes an additional parameter, the label $r$ of the state (i.e. article) to jump to. It should output an approximate eigenvector of the matrix $.55A_1 + .15A_2 + .3A_r$. Try to write it so that $A_r$ is not explicitly created. Remember to test your procedure on the Thimble-Wide Web before trying it on the big dataset.\n",
    "\n",
    "Compute the stationary distribution of the Markov chain that is biased towards `sport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method_biased(A1, A2, num_iterations, r):\n",
    "    A_biased = 0.55 * A1 + 0.15 * A2 + 0.3 * Mat(A1.D, {k: 1 for k in A1.f.keys() if k[0] == r})\n",
    "    return power_method(A_biased, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_biased_sports = power_method_biased(A1, A2, 6, 'sport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2007', '2006', '2005', 'paris', 'basketball']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikigoogle('jordan', 5, p_biased_sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['france', 'england', '2006', 'india', 'ireland']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikigoogle('tiger', 5, p_biased_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.13 Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What must be true of a matrix $A$ in order for $A$ to have an eigenvalue?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for a matrix $A$ to have an eigenvalue, it must be a square matrix such that there exists a scalar $\\lambda$ and a nonzero vector $\\boldsymbol{v}$ such that $A\\boldsymbol{v} = \\lambda\\boldsymbol{v}$.\n",
    "\n",
    "In particular, one special case is that any square matrix over $\\mathbb{C}$ has an eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are an eigenvalue and eigenvector of a matrix?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda$ is a scalar and $\\boldsymbol{v}$ is a nonzero vector such that $A\\boldsymbol{v} = \\lambda\\boldsymbol{v}$, then $\\lambda$ is an eigenvalue of $A$ and $\\boldsymbol{v}$ is a corresponding eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For what kind of problems are eigenvalues and eigenvectors useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are useful for problems that require many matrix multiplications by the same matrix (matrix exponentiation).  In the real world these types of problems often arise in state spaces that evolve over time according to consistent rules. (Discrete dynamic systems.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a diagonalizable matrix?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a matrix to be diagonalizable it must be similar to a diagonal matrix, meaning there is an invertible matrix $S$ such that $S^{-1}AS = \\Lambda$ where $\\Lambda$ is a diagonal matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an example of a matrix that has eigenvalues but is not diagonalizable?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}1 &1 &0\\\\0 &1 &1\\\\0 &0 &4\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Under what conditions is a matrix guaranteed to be diagonalizable?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an $n \\times n$ matrix has $n$ distinct eigenvalues, it is guaranteed to be diagonalizable.  Also, a symmetric matrix over $\\mathbb{R}$, an an upper-triangular matrix are both guaranteed to be diagonalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are some advantages of diagonalizable matrices?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonalizable matrices are easy to exponentiate since they can be decomposed into $S, S^{-1} and \\Lambda$ where $\\Lambda$ is a diagonal matrix, and thus only requires squaring the diagonal components to square the matrix.  This property also makes change of basis fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Under what conditions does a matrix have linearly independent eigenvectors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix has linearly independent eigenvectors if it is diagonalizable. Also, eigenvectors corresponding to _distinct_ eigenvalues are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the advantages to a matrix having linearly independent eigenvectors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a set of linearly independent eigenvectors that can span the column space of a matrix allows for a useful change of basis that allows for separation of linear transformations such that transformations by an eigenvector can be modeled simply by scaling by the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Under what conditions does a matrix have orthonormal eigenvectors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a matrix is symmetric it will have orthonormal eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the power method? What is it good for?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power method is a method for finding an approximation to the largest eigenvector (and thus the largest eigenvalue) of a matrix. It is used when finding an exact eigenvector would be very costly or impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the determinant?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant is the _signed_ area of the parallelogram (or more generally the signed volume of the parallelepiped for $n$ dimensions) created by $n$ vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the determinant relate to volumes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant is the _signed_ volume of a set of vectors, which negates certain dimensions based on vector orientation.  Otherwise it is identical to the volume!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which matrices have determinants?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All square matrices have determinants, but they may be zero or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which matrices have nonzero determinants?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix has a nonzero determinant if it is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do determinants have to do with eigenvalues?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers that make the characteristic polynomial of a square matrix equal to 0 are the iegenvalues of $A$, where the characteristic polynomial can be found with the determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Markov chain?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov chain is a state transition graph where the nodes are states and the directed edges represent transitions between states.  The transitions can be weighted based on a probability distribution where each node's outgoing edge weights are all >= 0 and sum to 1 (to form a valid distribution).\n",
    "\n",
    "A key characteristic of Markov chains is that their transition distribution at any node is not dependent on previous state - only on the current state and the transition distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do Markov chains have to do with eigenvectors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition function of a markov chain can be represented as a matrix where each entry $M[i, j]$ holds the transition probability from row $i$ to column $j$ (or vice versa depending on convention).\n",
    "\n",
    "With this representation, the normalized value of any of the largest eigenvector (or any scalar multiple) will give the stationary distribution of the Markov chain (the point of stability in the probability of \"landing\" on each node).  This will only be true, however, if the transition probability between _every_ pair of nodes is greater than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.14 Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.1:** For each matrix, find its eigenvalues and associated eigenvectors. Just use cleverness here; no algorithm should be needed.\n",
    "\n",
    "a) $\\begin{bmatrix}1 &2\\\\1 &0\\end{bmatrix}$\n",
    "\n",
    "$\\boldsymbol{v}_1 = [2,1], \\lambda_1= 2$  \n",
    "$\\boldsymbol{v}_2 = [-1,1], \\lambda_2= -1$\n",
    "\n",
    "b) $\\begin{bmatrix}1 &1\\\\3 &3\\end{bmatrix}$\n",
    "\n",
    "$\\boldsymbol{v}_1 = [1,3], \\lambda_1 = 4$  \n",
    "$\\boldsymbol{v}_2 = [-1,1], \\lambda_2 = 0$\n",
    "\n",
    "c) $\\begin{bmatrix}6 &0\\\\0 &6\\end{bmatrix}$\n",
    "\n",
    "$\\boldsymbol{v}_1 = [1,0], \\lambda_1 = 6$  \n",
    "$\\boldsymbol{v}_2 = [0,1], \\lambda_2 = 6$\n",
    "\n",
    "d) $\\begin{bmatrix}0 &4\\\\4 &0\\end{bmatrix}$\n",
    "\n",
    "$\\boldsymbol{v}_1 = [1,1], \\lambda_1 = 4$  \n",
    "$\\boldsymbol{v}_2 = [-1,1], \\lambda_2= -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.2:** In each of the following subproblems, we give you a matrix and some of its eigenvalues. Find a corresponding eigenvector.\n",
    "\n",
    "a) $\\begin{bmatrix}7 &-4\\\\2 &1\\end{bmatrix}$ and eigenvalues $\\lambda_1 = 5, \\lambda_2 = 3$\n",
    "\n",
    "For $\\lambda_1 = 5, \\boldsymbol{v}_1 = [2,1]$  \n",
    "For $\\lambda_2 = 3, \\boldsymbol{v}_2 = [1,1]$\n",
    "\n",
    "b) $\\begin{bmatrix}4 &0 &0\\\\2 &0 &3\\\\0 &1 &2\\end{bmatrix}$ and eigenvalues $\\lambda_1 = 3, \\lambda_2 = -1$\n",
    "\n",
    "For $\\lambda_1 = 3, \\boldsymbol{v}_1 = [0,1,1]$  \n",
    "For $\\lambda_2 = -1, \\boldsymbol{v}_2 = [0,3,-1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.3:** Given a matrix and its eigenvectors, find the corresponding eigenvalues:\n",
    "\n",
    "a) $\\begin{bmatrix}1 &2\\\\4 &3\\end{bmatrix}$ and $\\boldsymbol{v}_1 = [\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}]$ and $\\boldsymbol{v}_2 = [1, 2]$\n",
    "\n",
    "For $\\boldsymbol{v}_1 = [\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}]$, $\\lambda_1 = -1$  \n",
    "For $\\boldsymbol{v}_2 = [1, 2]$, $\\lambda_2 = 5$\n",
    "\n",
    "b) $\\begin{bmatrix}5 &0\\\\1 &2\\end{bmatrix}$ and $\\boldsymbol{v}_1 = [0, 1]$ and $\\boldsymbol{v}_2 = [3, 1]$\n",
    "\n",
    "For $\\boldsymbol{v}_1 = [0, 1]$, $\\lambda_1 = 2$  \n",
    "For $\\boldsymbol{v}_2 = [3, 1]$, $\\lambda_1 = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.4:** Let $A = \\begin{bmatrix}0 &-1\\\\1 &0\\end{bmatrix}$. Two (unnormalized) eigenvectors are $\\boldsymbol{v}_1 = \\begin{bmatrix}1\\\\\\boldsymbol{i}\\end{bmatrix}$ and $\\boldsymbol{v}_2 = \\begin{bmatrix}1\\\\-\\boldsymbol{i}\\end{bmatrix}$.\n",
    "\n",
    "1. Find the eigenvalue $\\lambda_1$ corresponding to eigenvector $\\boldsymbol{v}_1$, and show using matrix-vector multiplication that it is indeed the corresponding eigenvalue.\n",
    "\n",
    "  $\\begin{bmatrix}0 &-1\\\\1 &0\\end{bmatrix}\\begin{bmatrix}1\\\\\\boldsymbol{i}\\end{bmatrix} = \\begin{bmatrix}-\\boldsymbol{i}\\\\1\\end{bmatrix}$\n",
    "\n",
    "  $\\begin{bmatrix}-\\boldsymbol{i}\\\\1\\end{bmatrix} = \\lambda_1\\begin{bmatrix}1\\\\\\boldsymbol{i}\\end{bmatrix}$\n",
    "  \n",
    "  $\\lambda_1 = \\boldsymbol{i}^3$\n",
    "2. Find the eigenvalue $\\lambda_2$ corresponding to eigenvector $\\boldsymbol{v}_2$, and show using matrix-vector multiplication that it is indeed the corresponding eigenvalue.\n",
    "\n",
    "  $\\begin{bmatrix}0 &-1\\\\1 &0\\end{bmatrix}\\begin{bmatrix}1\\\\-\\boldsymbol{i}\\end{bmatrix} = \\begin{bmatrix}\\boldsymbol{i}\\\\1\\end{bmatrix}$\n",
    "\n",
    "  $\\begin{bmatrix}\\boldsymbol{i}\\\\1\\end{bmatrix} = \\lambda_2\\begin{bmatrix}1\\\\-\\boldsymbol{i}\\end{bmatrix}$\n",
    "  \n",
    "  $\\lambda_2 = \\boldsymbol{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.5:** Given a matrix $A$\n",
    "\n",
    "$\\begin{bmatrix}1 &2 &5 &7\\\\2 &9 &3 &7\\\\1 &0 &2 &2\\\\7 &3 &9 &1\\end{bmatrix}$\n",
    "\n",
    "a) Use the `power` method to approximate the eigenvector that corresonds to the eigenvalue $\\lambda_1$ of largest absolute value.\n",
    "\n",
    "b) Find an approximation to $\\lambda_1$.\n",
    "\n",
    "c) Using the `eig` procedure in `numpy`, find the eigenvalues of $A$.\n",
    "\n",
    "d) Compare your approximation to $\\lambda_1$ and the value of $\\lambda_1$ from part (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matutil import listlist2mat\n",
    "\n",
    "A = listlist2mat([[1,2,5,7],[2,9,3,7],[1,0,2,2],[7,3,9,1]])\n",
    "eig_1_est = power_method(A, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     0     1     2     3\n",
      "------------------------\n",
      " 0.394 0.792 0.105 0.454\n"
     ]
    }
   ],
   "source": [
    "print(eig_1_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_1 = (A * eig_1_est)[0] / eig_1_est[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0    1    2    3\n",
      "--------------------\n",
      " 5.68 11.4 1.51 6.54\n"
     ]
    }
   ],
   "source": [
    "print(lambda_1 * eig_1_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0    1    2    3\n",
      "--------------------\n",
      " 5.68 11.4 1.51 6.54\n"
     ]
    }
   ],
   "source": [
    "print(lambda_1 * eig_1_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.402834217884141\n"
     ]
    }
   ],
   "source": [
    "print(lambda_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eigs = np.linalg.eig(np.array([[1,2,5,7],[2,9,3,7],[1,0,2,2],[7,3,9,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -6.52407415,  14.40283422,  -0.42946982,   5.55070975]),\n",
       " array([[-0.57465082,  0.39436403,  0.74337056,  0.26767904],\n",
       "        [-0.25148857,  0.79218134, -0.20720852, -0.89716761],\n",
       "        [-0.11336817,  0.1049683 , -0.55766706,  0.2266157 ],\n",
       "        [ 0.77050477,  0.45377021,  0.30573237,  0.26848376]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.402834217884141,\n",
       " array([ 0.39436403,  0.79218134,  0.1049683 ,  0.45377021]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigs[0][1], eigs[1][:,1] # basically an exact match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_1 - eigs[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.6:** Prove:\n",
    "\n",
    "**Lemma 12.14.7:** Suppose $A$ is an invertible matrix. The eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\lambda$ be any eigenvalue of $A$ and $\\boldsymbol{v}$ be the corresponding eigenvector.\n",
    "\n",
    "Then $A\\boldsymbol{v} = \\lambda\\boldsymbol{v}$.\n",
    "\n",
    "Multiply both sides of this equation on the left by $A^{-1}$:\n",
    "\n",
    "$A^{-1}(A\\boldsymbol{v}) = A^{-1}(\\lambda\\boldsymbol{v})$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$\\boldsymbol{v} = \\lambda A^{-1}\\boldsymbol{v}$\n",
    "\n",
    "Multiplying both sides by $\\lambda^{-1}$,\n",
    "\n",
    "$\\lambda^{-1}\\boldsymbol{v} = A^{-1}\\boldsymbol{v}$\n",
    "\n",
    "$QED$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.8:** The lemma in Problem 12.14.6 shows that the eigenvalue of $A$ having smallest absolute value is the reciprocal of the eigenvalue of $A^{-1}$ having largest absolute value.\n",
    "\n",
    "How can you use the power method to obtain an estimate of the eigenvalue of $A$ having smallest absolute value? You should not compute the inverse of $A$. Instead, use another approach: solving a matrix equation.\n",
    "\n",
    "Use this approach on the matrix $A$ below:\n",
    "\n",
    "$A = \\begin{bmatrix}1 &2 &1 &9\\\\1 &3 &1 &3\\\\1 &2 &9 &5\\\\6 &4 &3 &1\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the power method to find the largest eigenvector $\\boldsymbol{v}_1$ of $A$, and then rather than multiply $A^{-1}\\boldsymbol{v}_1$ (which would mathematically give us what we want), we solve for the equation $A\\boldsymbol{x} = \\boldsymbol{v}_1$.  Then $\\boldsymbol{x}$ is the smallest eigenvector of $A$ (the largest eigenvector for $A^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_power(A, mu):\n",
    "    I = np.eye(A.shape[0])\n",
    "    v = np.random.randn(A.shape[0])\n",
    "    v = v / np.linalg.norm(v)\n",
    "    for i in range(100):\n",
    "        M = A - mu * I\n",
    "        w = np.linalg.solve(M, v)\n",
    "        v = w / np.linalg.norm(w)\n",
    "        mu = v.dot(A.dot(v))\n",
    "        if (A.dot(v) == mu * v).all():\n",
    "            break\n",
    "    return mu, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,1,9],[1,3,1,3],[1,2,9,5],[6,4,3,1]])\n",
    "mu, v = inverse_power(A, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5234715699816992,\n",
       " array([ -5.54935979e-01,   8.04661145e-01,   1.54132036e-04,\n",
       "         -2.11107737e-01]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5234715699817003,\n",
       " array([  5.54935979e-01,  -8.04661145e-01,  -1.54132036e-04,\n",
       "          2.11107737e-01]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "eigenvalues[-1], eigenvectors[:, -1] # exact match!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.9:** Let $k$ be a number, $A$ an $n \\times n$ matrix and $I$ the identity matrix. Let $\\lambda_1, ..., \\lambda_m (m \\leq n)$ be the eigenvalues of $A$. What are the eigenvalues of $A - kI$? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalutes of $A - kI$ are the eigenvalues of $A$ shifted down by $k$:\n",
    "\n",
    "$\\begin{align}\n",
    "(A - kI)\\boldsymbol{v}_i &=A\\boldsymbol{v}_i - kI\\boldsymbol{v}\\\\&=\\lambda_i\\boldsymbol{v}_i - k\\boldsymbol{v}_i\\\\&=(\\lambda_i - k)\\boldsymbol{v}_i\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.10:** How can you use the lemma in Problem 12.14.8 and the result from 12.14.9 to address the following computational problem?\n",
    "\n",
    "* _input:_ a matrix $A$ and a value $k$ that is an estimate of an eigenvalue $\\lambda_i$ of $A$ (and is closer to $\\lambda_i$ than to any other eigenvalue of $A$)\n",
    "* _output:_ an even better estimate of that eigenvalue.\n",
    "\n",
    "Show how to use this method on the following data:\n",
    "\n",
    "$A = \\begin{bmatrix}3 &0 &1\\\\4 &8 &1\\\\9 &0 &0\\end{bmatrix}$, $k = 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually the above method _does_ already do that (since it is frankly taken directly from the slides to solve exactly this problem and adapted for numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.8541019662496847, array([ 0.35577221, -0.66204524,  0.65963796]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[3,0,1],[4,8,1],[9,0,0]])\n",
    "inverse_power(A, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.8541019662496847, array([ 0.35577221, -0.66204524,  0.65963796]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "eigenvalues[1], eigenvectors[:, 1] # corresopnds to the second eigenvector/eigenvalue (down to insane precision since we're doing 100 iterations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 12.14.11:** (See weather Markov chain in book)\n",
    "\n",
    "a) Give the transition matrix $A$ with row and column labels `{'S', 'R', 'F', 'W'}` (for \"Sunny\", \"Rainy\", \"Foggy\", and \"Windy\"). Entry $(i, j)$ of $A$ should be the probability of transitioning from state $j$ to state $i$. Construct a `Mat` in Python representing this matrix.\n",
    "\n",
    "b) A probability distribution can be represented by a `{'S', 'R', 'F', 'W'}`-vector. If $\\boldsymbol{v}$ is the probability distribution of the weather for today then $A\\boldsymbol{v}$ is the probability distribution of the weather for tomorrow.\n",
    "\n",
    "Write down the vector $\\boldsymbol{v}$ representing the probability distribution in which it is Windy with probability 1. Calculate the vector $A\\boldsymbol{v}$, which gives the probability distribution for the following day. Does it make sense in view of the diagram?\n",
    "\n",
    "c) Write down the vector $\\boldsymbol{v}$ representing the uniform distribution. calculate $A\\boldsymbol{v}$.\n",
    "\n",
    "d) What is the probability distribution over the weather states in 400 days given that the start probability distribution is uniform over the states?\n",
    "\n",
    "e) Based on previous parts, name one eigenvalue and a corresponding eigenvector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mat import Mat\n",
    "D = {'S', 'R', 'F', 'W'}\n",
    "A = Mat((D, D), {\n",
    "    ('S','S'): 0.5, ('R','S'): 0.2, ('W', 'S'): 0.3,\n",
    "    ('S','R'): 0.2, ('R','R'): 0.6, ('F', 'R'): 0.2,\n",
    "    ('F','F'): 0.4, ('W','F'): 0.6,\n",
    "    ('S','W'): 0.1, ('F','W'): 0.8, ('W', 'W'): 0.1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   F R   S   W\n",
      "--------------\n",
      " 0.8 0 0.1 0.1\n"
     ]
    }
   ],
   "source": [
    "from vec import Vec\n",
    "\n",
    "v = Vec(D, {'W': 1})\n",
    "\n",
    "v_2 = A * v # distribution of tomorrow given windy today\n",
    "print(v_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are exactly the transition probabilities from the 'Windy' node in the diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    F    R    S    W\n",
      "--------------------\n",
      " 0.25 0.25 0.25 0.25\n"
     ]
    }
   ],
   "source": [
    "uniform_dist = Vec(D, {k: 1 / len(D) for k in D})\n",
    "print(uniform_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    F   R   S    W\n",
      "------------------\n",
      " 0.35 0.2 0.2 0.25\n"
     ]
    }
   ],
   "source": [
    "print(A * uniform_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     F      R     S    W\n",
      "------------------------\n",
      " 0.798 0.0725 0.145 0.58\n"
     ]
    }
   ],
   "source": [
    "# inlining `power_method` to use an explicit starting vector.\n",
    "x = uniform_dist\n",
    "for i in range(400):\n",
    "    x = A * x\n",
    "    norm = (x * x) ** 0.5\n",
    "    x = x / norm\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, one eigenvector (the largest eigenvector) is\n",
    "\n",
    "|     F|      R|     S|    W\n",
    "|------------------------\n",
    "| 0.798| 0.0725| 0.145| 0.58\n",
    "\n",
    "A corresponding eigenvalue is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A * x)['R'] / x['R']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.0 (which we already knew!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
