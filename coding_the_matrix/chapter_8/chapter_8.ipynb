{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8\n",
    "# The Inner Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8.3.4:** Demonstrate using a numerical example that Lemma 8.3.3 would not be true if we remove the requirement that $\\boldsymbol{u}$ and $\\boldsymbol{v}$ are orthogonal.\n",
    "\n",
    "Lemma 8.3.3: If $\\boldsymbol{u}$ is orthogonal to $\\boldsymbol{v}$ then, for any scalars $\\alpha$, $\\beta$,\n",
    "\n",
    "$\\|\\alpha\\boldsymbol{u} + \\beta\\boldsymbol{v}\\|^2 = \\alpha^2\\|\\boldsymbol{u}\\|^2 + \\beta^2\\|\\boldsymbol{v}\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the non-orthogonal 2-vectors $\\boldsymbol{v} = [1,0]$, $\\boldsymbol{u} = [3,4]$ and the scalars $\\alpha = 2, \\beta = 2$.  Then  \n",
    "$\\begin{align}\n",
    "\\|\\alpha\\boldsymbol{u} + \\beta\\boldsymbol{v}\\|^2 &= (\\alpha\\boldsymbol{u}  + \\beta\\boldsymbol{v}) \\cdot(\\alpha\\boldsymbol{u}  + \\beta\\boldsymbol{v})\\\\\n",
    "&= 2^2([3,4]\\cdot[3,4]) + 2^2([1,0]\\cdot[1,0]) + 2 \\cdot 2([3,4]\\cdot[1,0])+2\\cdot 2([1,0]\\cdot[3,4])\\\\\n",
    "&= 4 \\cdot 25 + 4 \\cdot 1 + 4 \\cdot 3 + 4 \\cdot 3\\\\\n",
    "&\\neq \\alpha^2\\|\\boldsymbol{u}\\|^2 + \\beta^2\\|\\boldsymbol{v}\\|^2 = 4 \\cdot 25 + 4 \\cdot 1\n",
    "\\end{align}$\n",
    "\n",
    "$QED$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8.3.5:** Using induction and Lemma 8.3.3, prove the following generalization: Suppose $\\boldsymbol{v}_1, ..., \\boldsymbol{v}_n$ are mutually orthogonal. For any coefficients $\\alpha_1, ..., \\alpha_n$,\n",
    "\n",
    "$\\|\\alpha_1\\boldsymbol{v}_1 + \\cdots + \\alpha_n\\boldsymbol{v}_n\\|^2 = \\alpha_1^2\\|\\boldsymbol{v}_1\\|^2 + \\cdots + \\alpha_n^2\\|\\boldsymbol{v}_n\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using induction on $n$,\n",
    "\n",
    "consider when $n = 0$.  Then\n",
    "\n",
    "$\\|\\{\\}\\|^2 = 0^2 = 0$\n",
    "\n",
    "Assume the generalization holds for $n = k$ and we prove for the case $n = k + 1$.\n",
    "\n",
    "$\\begin{align}\n",
    "\\|\\alpha_1\\boldsymbol{v}_1 + \\cdots + \\alpha_k\\boldsymbol{v}_k + \\alpha_{k+1}\\boldsymbol{v}_{k+1}\\|^2 &=(\\alpha_1\\boldsymbol{v}_1 + \\cdots + \\alpha_k\\boldsymbol{v}_k + \\alpha_{k+1}\\boldsymbol{v}_{k+1})(\\alpha_1\\boldsymbol{v}_1 + \\cdots + \\alpha_k\\boldsymbol{v}_k + \\alpha_{k+1}\\boldsymbol{v}_{k+1})\\\\\n",
    "&= (\\alpha_1\\boldsymbol{v}_1 + \\cdots + \\alpha_k\\boldsymbol{v}_k)(\\alpha_1\\boldsymbol{v}_1 + \\cdots + \\alpha_k\\boldsymbol{v}_k) + \\alpha_{k+1}\\boldsymbol{v}_{k+1} \\cdot \\alpha_{k+1}\\boldsymbol{v}_{k+1} \\space\\text{(Since}\\space \\boldsymbol{v}_1, ..., \\boldsymbol{v}_{k+1} \\text{are mutually orthogonal)}\\\\\n",
    "&= \\|\\alpha_1\\boldsymbol{v}_1 + \\cdots + \\alpha_k\\boldsymbol{v}_k\\|^2 + \\alpha_{k+1}\\boldsymbol{v}_{k+1} \\cdot \\alpha_{k+1}\\boldsymbol{v}_{k+1}\\\\\n",
    "&= \\alpha_1^2\\|\\boldsymbol{v}_1\\|^2 + \\cdots + \\alpha_k^2\\|\\boldsymbol{v}_k\\|^2 + \\alpha_{k+1}\\boldsymbol{v}_{k+1} \\cdot \\alpha_{k+1}\\boldsymbol{v}_{k+1}\\space\\text{using the induction assumption for }k\\\\\n",
    "&= \\alpha_1^2\\|\\boldsymbol{v}_1\\|^2 + \\cdots + \\alpha_k^2\\|\\boldsymbol{v}_k\\|^2 + \\alpha_{k+1}^2\\|\\boldsymbol{v}_{k+1}\\|^2\n",
    "\\end{align}$\n",
    "\n",
    "$QED$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from vec import Vec\n",
    "from vecutil import list2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_along(b, v):\n",
    "    sigma = ((b * v) / (v * v)) if v * v > 1e-20 else 0\n",
    "    return sigma * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({0, 1},{0: 3.0, 1: 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_along(list2vec([2, 4]), list2vec([6, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8.3.15:** Write a Python procedure `projection_matrix(v)` that, given a vector $\\boldsymbol{v}$, returns the matrix $M$ such that $\\pi_\\boldsymbol{v}(\\boldsymbol{x}) = M\\boldsymbol{x}$. Your procedure should be correct even if $\\|\\boldsymbol{v}\\| \\neq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mat import Mat\n",
    "from matutil import coldict2mat\n",
    "\n",
    "def outer_product(x, v):\n",
    "    return coldict2mat([x]) * coldict2mat([v]).transpose()\n",
    "\n",
    "def projection_matrix(v):\n",
    "    return outer_product(v, v) * (1 / (v * v)) # element-wise division by squared norm (dot-product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({0, 1},{0: 3.0, 1: 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection_matrix(list2vec([6, 2])) * list2vec([2, 4]) # same result as above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8.3.16:** Suppose $\\boldsymbol{v}$ is a nonzero $n$-vector. What is the rank of the matrix $M$ such that $\\pi_\\boldsymbol{v}(\\boldsymbol{x}) = M\\boldsymbol{x}$? Explain your answer using appropriate interpretations of matrix-vector or matrix-matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $M$ is a scalar multiple of the outer product of $\\boldsymbol{v}$, it will have the same rank as the outer product of $\\boldsymbol{v}$.  Since the outer product of $\\boldsymbol{v}$ is the matrix-matrix multiplication of column-vector $\\boldsymbol{v}$ with row-vector $\\boldsymbol{v}^T$, by the linear-combinations definition of matrix-matrix multiplication, each row of $M$ will be a linear combination of $\\boldsymbol{v}^T$ with the corresponding element of the column-vector $\\boldsymbol{v}$. Since this is just a single element, each row of $M$ will be a scalar multiple of $\\boldsymbol{v}^T$.  Thus, $rank(M) = rank(\\boldsymbol{v})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Lab: machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.1:** Use `read_training_data` to read the data in the file `train.data` into the variables `A`, `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cancer_data import read_training_data\n",
    "\n",
    "A, b = read_training_data('train.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.2:** Write a procedure `signum(u)` with the following spec:\n",
    "\n",
    "* _input:_ a `Vec` `u`\n",
    "* _output:_ the `Vec` `v` with the same domain as `u` such that `v[d] = 1 if u[d] >= 0 else -1`.\n",
    "\n",
    "For examples, `signum(Vec({'A', 'B'}, {'A': 3, 'B': -2}))` is `Vec({'A', 'B'}, {'A': 1, 'B': -1})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signum(u):\n",
    "    return Vec(u.D, {k: 1 if v >= 0 else -1 for k, v in u.f.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({'A', 'B'},{'A': 1, 'B': -1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signum(Vec({'A', 'B'}, {'A': 3, 'B': -2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.3:** Write the procedure `fraction_wrong(A, b, w)` with the following spec:\n",
    "\n",
    "* _input:_ An $R \\times C$ matrix $A$ whose rows are feature vectors, an $R$-vector $\\boldsymbol{b}$ whose entries are $+1$ and $-1$, and a $C$-vector $\\boldsymbol{w}$\n",
    "* _output:_ The fraction of row labels $r$ of $A$ such that the sign of (row $r$ of $A$) $\\cdot \\boldsymbol{w}$ differs from that of $\\boldsymbol{b}[r]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the book hints that there's a \"clever\" way to write this\n",
    "# without explicit loops using matrix-vector multiplication, \n",
    "# dot-product and the `signum` procedure.\n",
    "# I'm taking \"no loops\" to also mean no comprehensions.\n",
    "# This is the purest way I could find, but it's not very intuitive.\n",
    "# Here's the algebra:\n",
    "#     num_right + num_wrong = len\n",
    "#     num_right - num_wrong = signum(pred) * actual\n",
    "#     => num_wrong = (len - signum(pred) * actual) / 2\n",
    "#     => frac_wrong = num_wrong/len = 1/2 * (1 - (signum(pred) * actual) / len)\n",
    "def fraction_wrong(A, b, w):\n",
    "    return 0.5 * (1 - (signum(A * w) * b) / len(b.D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5133333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction_wrong(A, b, Vec(A.D[1], {k: 1 for k in A.D[1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5133333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in b.f.values() if x == -1]) / len(b.f) # just checking :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.4:** Write a procedure `loss(A, b, w)` that takes as input the training data $A$, $\\boldsymbol{b}$ and a hypothesis vector $\\boldsymbol{w}$, and returns the value $L(\\boldsymbol{w})$ of the loss function for input $\\boldsymbol{w}$.\n",
    "\n",
    "Find the value of the loss function at a simple hypothesis vector such as the all-ones vector or a random vector of +1's and -1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(A, b, w):\n",
    "    error = (A * w - b)\n",
    "    return error * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461169191.1916513"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(A, b, Vec(A.D[1], {k: 1 for k in A.D[1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.9:** Write a procedure `find_grad(A, b, w)` that takes as input the training data $A$, $\\boldsymbol{b}$ and a hypothesis vector $\\boldsymbol{w}$ and returns the value of the gradient of $L$ at the point $\\boldsymbol{w}$, using the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_\\limits{i=1}^\\limits{m}2(\\boldsymbol{a}_i \\cdot \\boldsymbol{w} - b_i)\\boldsymbol{a}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_grad(A, b, w):\n",
    "    return 2 * (A * w - b) * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({'compactness(stderr)', 'texture(mean)', 'radius(mean)', 'smoothness(stderr)', 'concave points(mean)', 'symmetry(worst)', 'area(stderr)', 'concave points(stderr)', 'perimeter(mean)', 'symmetry(mean)', 'concavity(stderr)', 'compactness(worst)', 'fractal dimension(stderr)', 'symmetry(stderr)', 'area(mean)', 'smoothness(mean)', 'area(worst)', 'fractal dimension(worst)', 'perimeter(worst)', 'compactness(mean)', 'radius(stderr)', 'texture(worst)', 'concavity(worst)', 'concave points(worst)', 'concavity(mean)', 'fractal dimension(mean)', 'smoothness(worst)', 'radius(worst)', 'perimeter(stderr)', 'texture(stderr)'},{'compactness(stderr)': 34491.53912591444, 'texture(mean)': 23824902.07755112, 'radius(mean)': 19061204.41971144, 'smoothness(stderr)': 8048.720134830324, 'concave points(mean)': 83000.61553163151, 'symmetry(worst)': 359563.8861937369, 'area(stderr)': 74089316.99183664, 'concave points(stderr)': 15786.076442543796, 'perimeter(mean)': 125171204.03328276, 'symmetry(mean)': 220058.13877246578, 'concavity(stderr)': 43578.22090981913, 'compactness(worst)': 366787.53378183977, 'fractal dimension(stderr)': 4634.864505998124, 'symmetry(stderr)': 24387.414899865937, 'area(mean)': 1015156399.7450641, 'smoothness(mean)': 116294.04324286385, 'area(worst)': 1450798295.3343, 'fractal dimension(worst)': 101379.40753281576, 'perimeter(worst)': 150832057.82594767, 'compactness(mean)': 147514.83121069864, 'radius(stderr)': 640442.4350492798, 'texture(worst)': 31994327.921173435, 'concavity(worst)': 419181.55922271876, 'concave points(worst)': 175761.59840507226, 'concavity(mean)': 149655.04588915987, 'fractal dimension(mean)': 72765.72185727661, 'smoothness(worst)': 161234.48342337078, 'radius(worst)': 22714731.107052494, 'perimeter(stderr)': 4541111.794060581, 'texture(stderr)': 1410905.334191034})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_grad(A, b, Vec(A.D[1], {k: 1 for k in A.D[1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.10:** Write a procedure `gradient_descent_step(A, b, w, sigma)` that, given the training data $A$, $\\boldsymbol{b}$ and the current hypothesis vector $\\boldsymbol{w}$, returns the next hypothesis vector.\n",
    "\n",
    "The next hypothesis vector is obtained by computing the gradient, multiplying the gradient by the step size, and subtracting the result from the current hypothesis vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(A, b, w, sigma):\n",
    "    return w - sigma * find_grad(A, b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.11:** Write a procedure `gradient_descent(A, b, w, sigma, T)` that takes as input the training data $A$, $\\boldsymbol{b}$, and initial value $\\boldsymbol{w}$ for the hypothesis vector, a step size $\\sigma$, and a number $T$ of iterations. The procedure should implement gradient descent as described above for $T$ iterations, and return the final value of $\\boldsymbol{w}$.\n",
    "\n",
    "Every thirty iterations or so, the procedure should print out the value of the loss function and the fraction wrong for the current hypothesis vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(A, b, w, sigma, T):\n",
    "    for i in range(T):\n",
    "        if i % 30 == 0:\n",
    "            print('Loss:\\t', loss(A, b, w))\n",
    "            print('Fraction wrong:\\t', fraction_wrong(A, b, w))\n",
    "        w = gradient_descent_step(A, b, w, sigma)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.12:** Try out your gradient descent code on the training data! Notice that the fraction wrong might go up even while the value of the loss function goes down. Eventually, as the value of the loss function continues to decrease, the fraction wrong should also decrease (up to a point).\n",
    "\n",
    "Try a step size of $\\sigma = 2 \\cdot 10^{-9}$, then try a step size of $\\sigma = 10^{-9}$.\n",
    "\n",
    "Try starting with the all-ones vector. Then try starting with the zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\t 1461169191.1916513\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 31801400738349.836\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 6.932905202112942e+17\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 1.5114169719634027e+22\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 3.294984132260201e+26\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 7.183272805083588e+30\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 1.5659986853065471e+35\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 3.4139757028945487e+39\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 7.442681918773589e+43\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 1.6225515048942348e+48\n",
      "Fraction wrong:\t 0.5133333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vec({'compactness(stderr)', 'texture(mean)', 'radius(mean)', 'smoothness(stderr)', 'concave points(mean)', 'symmetry(worst)', 'area(stderr)', 'concave points(stderr)', 'perimeter(mean)', 'symmetry(mean)', 'concavity(stderr)', 'compactness(worst)', 'fractal dimension(stderr)', 'symmetry(stderr)', 'area(mean)', 'smoothness(mean)', 'area(worst)', 'fractal dimension(worst)', 'perimeter(worst)', 'compactness(mean)', 'radius(stderr)', 'texture(worst)', 'concavity(worst)', 'concave points(worst)', 'concavity(mean)', 'fractal dimension(mean)', 'smoothness(worst)', 'radius(worst)', 'perimeter(stderr)', 'texture(stderr)'},{'compactness(stderr)': 1.528369278607165e+17, 'texture(mean)': 1.0585263085778515e+20, 'radius(mean)': 8.511769465095394e+19, 'smoothness(stderr)': 3.5395650399635012e+16, 'concave points(mean)': 3.737791849292671e+17, 'symmetry(worst)': 1.5954234188339487e+18, 'area(stderr)': 3.340090771814811e+20, 'concave points(stderr)': 7.004450798487749e+16, 'perimeter(mean)': 5.591410841397138e+20, 'symmetry(mean)': 9.746038173824347e+17, 'concavity(stderr)': 1.930048447793362e+17, 'compactness(worst)': 1.6405487803450621e+18, 'fractal dimension(stderr)': 2.040723954712695e+16, 'symmetry(stderr)': 1.072938221280838e+17, 'area(mean)': 4.5680040046922286e+21, 'smoothness(mean)': 5.150245792257523e+17, 'area(worst)': 6.562980149100985e+21, 'fractal dimension(worst)': 4.492736379625264e+17, 'perimeter(worst)': 6.755788478696378e+20, 'compactness(mean)': 6.581863751759126e+17, 'radius(stderr)': 2.867847410205043e+18, 'texture(worst)': 1.4228674048890354e+20, 'concavity(worst)': 1.8812548404675758e+18, 'concave points(worst)': 7.894084767931523e+17, 'concavity(mean)': 6.724381688712358e+17, 'fractal dimension(mean)': 3.21576799661755e+17, 'smoothness(worst)': 7.150819367157926e+17, 'radius(worst)': 1.0171492698295163e+20, 'perimeter(stderr)': 2.032503012320818e+19, 'texture(stderr)': 6.215980550318182e+18})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Vec(A.D[1], {k: 1 for k in A.D[1]})\n",
    "sigma = 2e-9 # sigma too large\n",
    "T = 300\n",
    "gradient_descent(A, b, w, sigma, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with this value of sigma, we keep overstepping and end up doing worse as the iterations continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\t 1461169191.1916513\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 1867476.164994827\n",
      "Fraction wrong:\t 0.73\n",
      "Loss:\t 1501858.2431936574\n",
      "Fraction wrong:\t 0.7133333333333334\n",
      "Loss:\t 1261341.0368251363\n",
      "Fraction wrong:\t 0.7133333333333334\n",
      "Loss:\t 1099845.1036712618\n",
      "Fraction wrong:\t 0.7166666666666667\n",
      "Loss:\t 988408.6940531735\n",
      "Fraction wrong:\t 0.7133333333333334\n",
      "Loss:\t 908821.9038339362\n",
      "Fraction wrong:\t 0.71\n",
      "Loss:\t 849628.557730042\n",
      "Fraction wrong:\t 0.7166666666666667\n",
      "Loss:\t 803615.1441978435\n",
      "Fraction wrong:\t 0.7033333333333334\n",
      "Loss:\t 766233.6931558683\n",
      "Fraction wrong:\t 0.7066666666666667\n",
      "Loss:\t 734611.217512566\n",
      "Fraction wrong:\t 0.7066666666666667\n",
      "Loss:\t 706927.5476219974\n",
      "Fraction wrong:\t 0.7033333333333334\n",
      "Loss:\t 682024.5367277957\n",
      "Fraction wrong:\t 0.6966666666666667\n",
      "Loss:\t 659160.5808964253\n",
      "Fraction wrong:\t 0.6933333333333334\n",
      "Loss:\t 637856.4056941518\n",
      "Fraction wrong:\t 0.6933333333333334\n",
      "Loss:\t 617798.174904544\n",
      "Fraction wrong:\t 0.69\n",
      "Loss:\t 598776.6022666126\n",
      "Fraction wrong:\t 0.69\n",
      "Loss:\t 580648.6767557618\n",
      "Fraction wrong:\t 0.69\n",
      "Loss:\t 563313.5920691204\n",
      "Fraction wrong:\t 0.6833333333333333\n",
      "Loss:\t 546697.598749201\n",
      "Fraction wrong:\t 0.6833333333333333\n",
      "Loss:\t 530744.4617764843\n",
      "Fraction wrong:\t 0.6799999999999999\n",
      "Loss:\t 515409.4402019698\n",
      "Fraction wrong:\t 0.6799999999999999\n",
      "Loss:\t 500655.4802437102\n",
      "Fraction wrong:\t 0.6833333333333333\n",
      "Loss:\t 486450.7999207163\n",
      "Fraction wrong:\t 0.6833333333333333\n",
      "Loss:\t 472767.3489409788\n",
      "Fraction wrong:\t 0.6799999999999999\n",
      "Loss:\t 459579.8195230846\n",
      "Fraction wrong:\t 0.6766666666666666\n",
      "Loss:\t 446865.0043964367\n",
      "Fraction wrong:\t 0.67\n",
      "Loss:\t 434601.37394963525\n",
      "Fraction wrong:\t 0.67\n",
      "Loss:\t 422768.7920585803\n",
      "Fraction wrong:\t 0.67\n",
      "Loss:\t 411348.3200002417\n",
      "Fraction wrong:\t 0.67\n",
      "Loss:\t 400322.0766236512\n",
      "Fraction wrong:\t 0.67\n",
      "Loss:\t 389673.134738183\n",
      "Fraction wrong:\t 0.67\n",
      "Loss:\t 379385.4410856544\n",
      "Fraction wrong:\t 0.67\n",
      "Loss:\t 369443.75191695034\n",
      "Fraction wrong:\t 0.6666666666666666\n",
      "Loss:\t 359833.5791193201\n",
      "Fraction wrong:\t 0.6633333333333333\n",
      "Loss:\t 350541.1436801109\n",
      "Fraction wrong:\t 0.6633333333333333\n",
      "Loss:\t 341553.33443021605\n",
      "Fraction wrong:\t 0.66\n",
      "Loss:\t 332857.67073955026\n",
      "Fraction wrong:\t 0.66\n",
      "Loss:\t 324442.2682966573\n",
      "Fraction wrong:\t 0.66\n",
      "Loss:\t 316295.8073951549\n",
      "Fraction wrong:\t 0.6533333333333333\n",
      "Loss:\t 308407.5033339738\n",
      "Fraction wrong:\t 0.6533333333333333\n",
      "Loss:\t 300767.07865570125\n",
      "Fraction wrong:\t 0.6533333333333333\n",
      "Loss:\t 293364.73702257685\n",
      "Fraction wrong:\t 0.6533333333333333\n",
      "Loss:\t 286191.1385784462\n",
      "Fraction wrong:\t 0.6433333333333333\n",
      "Loss:\t 279237.3766769417\n",
      "Fraction wrong:\t 0.6366666666666667\n",
      "Loss:\t 272494.95587758033\n",
      "Fraction wrong:\t 0.6333333333333333\n",
      "Loss:\t 265955.7711261529\n",
      "Fraction wrong:\t 0.6333333333333333\n",
      "Loss:\t 259612.0880461702\n",
      "Fraction wrong:\t 0.6366666666666667\n",
      "Loss:\t 253456.52427578787\n",
      "Fraction wrong:\t 0.6333333333333333\n",
      "Loss:\t 247482.0317904674\n",
      "Fraction wrong:\t 0.6333333333333333\n",
      "Loss:\t 241681.88015631534\n",
      "Fraction wrong:\t 0.63\n",
      "Loss:\t 236049.6406629126\n",
      "Fraction wrong:\t 0.63\n",
      "Loss:\t 230579.17128777227\n",
      "Fraction wrong:\t 0.6333333333333333\n",
      "Loss:\t 225264.60244749245\n",
      "Fraction wrong:\t 0.63\n",
      "Loss:\t 220100.32349332006\n",
      "Fraction wrong:\t 0.63\n",
      "Loss:\t 215080.9699112377\n",
      "Fraction wrong:\t 0.6233333333333333\n",
      "Loss:\t 210201.41118893342\n",
      "Fraction wrong:\t 0.6233333333333333\n",
      "Loss:\t 205456.73931407667\n",
      "Fraction wrong:\t 0.6233333333333333\n",
      "Loss:\t 200842.2578702685\n",
      "Fraction wrong:\t 0.6266666666666667\n",
      "Loss:\t 196353.47169887004\n",
      "Fraction wrong:\t 0.6233333333333333\n",
      "Loss:\t 191986.0770966148\n",
      "Fraction wrong:\t 0.6233333333333333\n",
      "Loss:\t 187735.95252055422\n",
      "Fraction wrong:\t 0.6233333333333333\n",
      "Loss:\t 183599.14977339085\n",
      "Fraction wrong:\t 0.6166666666666667\n",
      "Loss:\t 179571.88564372718\n",
      "Fraction wrong:\t 0.6166666666666667\n",
      "Loss:\t 175650.5339771044\n",
      "Fraction wrong:\t 0.6166666666666667\n",
      "Loss:\t 171831.6181550091\n",
      "Fraction wrong:\t 0.6166666666666667\n",
      "Loss:\t 168111.80396024836\n",
      "Fraction wrong:\t 0.6166666666666667\n"
     ]
    }
   ],
   "source": [
    "sigma = 1e-9\n",
    "T = 2000\n",
    "w = gradient_descent(A, b, w, sigma, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:\t 165685.39935656055\n",
      "Final fraction wrong:\t 0.6166666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Final loss:\\t', loss(A, b, w))\n",
    "print('Final fraction wrong:\\t', fraction_wrong(A, b, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is worse than what we started with (the all-ones guess)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\t 300.0\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 251.01076079619205\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 239.28149884833175\n",
      "Fraction wrong:\t 0.5133333333333333\n",
      "Loss:\t 231.18126788323136\n",
      "Fraction wrong:\t 0.43\n",
      "Loss:\t 225.39735401131654\n",
      "Fraction wrong:\t 0.26666666666666666\n",
      "Loss:\t 221.10335403821537\n",
      "Fraction wrong:\t 0.18333333333333335\n",
      "Loss:\t 217.77839578627712\n",
      "Fraction wrong:\t 0.13666666666666666\n",
      "Loss:\t 215.09359415175376\n",
      "Fraction wrong:\t 0.09999999999999998\n",
      "Loss:\t 212.84073473769675\n",
      "Fraction wrong:\t 0.08000000000000002\n",
      "Loss:\t 210.8874787199391\n",
      "Fraction wrong:\t 0.08333333333333331\n",
      "Loss:\t 209.14922459274135\n",
      "Fraction wrong:\t 0.08000000000000002\n",
      "Loss:\t 207.57143149149456\n",
      "Fraction wrong:\t 0.08000000000000002\n",
      "Loss:\t 206.11851313090224\n",
      "Fraction wrong:\t 0.08666666666666667\n",
      "Loss:\t 204.76685863470976\n",
      "Fraction wrong:\t 0.08333333333333331\n",
      "Loss:\t 203.5004454685678\n",
      "Fraction wrong:\t 0.09000000000000002\n",
      "Loss:\t 202.30808054345363\n",
      "Fraction wrong:\t 0.08666666666666667\n",
      "Loss:\t 201.1816640826094\n",
      "Fraction wrong:\t 0.08666666666666667\n",
      "Loss:\t 200.11509601682266\n",
      "Fraction wrong:\t 0.08666666666666667\n",
      "Loss:\t 199.10358609218875\n",
      "Fraction wrong:\t 0.09000000000000002\n",
      "Loss:\t 198.1432176931539\n",
      "Fraction wrong:\t 0.09000000000000002\n",
      "Loss:\t 197.23067116652766\n",
      "Fraction wrong:\t 0.09000000000000002\n",
      "Loss:\t 196.36304746697417\n",
      "Fraction wrong:\t 0.09333333333333332\n",
      "Loss:\t 195.5377549484791\n",
      "Fraction wrong:\t 0.09000000000000002\n",
      "Loss:\t 194.75243594629947\n",
      "Fraction wrong:\t 0.09333333333333332\n",
      "Loss:\t 194.0049184739146\n",
      "Fraction wrong:\t 0.09333333333333332\n",
      "Loss:\t 193.2931838113651\n",
      "Fraction wrong:\t 0.09999999999999998\n",
      "Loss:\t 192.61534418574803\n",
      "Fraction wrong:\t 0.10333333333333333\n",
      "Loss:\t 191.969626895665\n",
      "Fraction wrong:\t 0.10333333333333333\n",
      "Loss:\t 191.35436258268288\n",
      "Fraction wrong:\t 0.10999999999999999\n",
      "Loss:\t 190.76797620185036\n",
      "Fraction wrong:\t 0.11333333333333334\n",
      "Loss:\t 190.20897977677336\n",
      "Fraction wrong:\t 0.11333333333333334\n",
      "Loss:\t 189.67596636010907\n",
      "Fraction wrong:\t 0.10999999999999999\n",
      "Loss:\t 189.16760483117085\n",
      "Fraction wrong:\t 0.11333333333333334\n",
      "Loss:\t 188.6826352950353\n",
      "Fraction wrong:\t 0.11333333333333334\n",
      "Loss:\t 188.219864931089\n",
      "Fraction wrong:\t 0.11666666666666664\n",
      "Loss:\t 187.77816419165558\n",
      "Fraction wrong:\t 0.11666666666666664\n",
      "Loss:\t 187.3564632846535\n",
      "Fraction wrong:\t 0.11666666666666664\n",
      "Loss:\t 186.9537488953413\n",
      "Fraction wrong:\t 0.11666666666666664\n",
      "Loss:\t 186.56906111566045\n",
      "Fraction wrong:\t 0.11666666666666664\n",
      "Loss:\t 186.20149055830245\n",
      "Fraction wrong:\t 0.12\n",
      "Loss:\t 185.85017563821523\n",
      "Fraction wrong:\t 0.12\n",
      "Loss:\t 185.5143000079157\n",
      "Fraction wrong:\t 0.12333333333333335\n",
      "Loss:\t 185.1930901354384\n",
      "Fraction wrong:\t 0.12666666666666665\n",
      "Loss:\t 184.88581301541603\n",
      "Fraction wrong:\t 0.12333333333333335\n",
      "Loss:\t 184.59177400498814\n",
      "Fraction wrong:\t 0.12333333333333335\n",
      "Loss:\t 184.3103147770976\n",
      "Fraction wrong:\t 0.12333333333333335\n",
      "Loss:\t 184.04081138440984\n",
      "Fraction wrong:\t 0.12666666666666665\n",
      "Loss:\t 183.78267242761257\n",
      "Fraction wrong:\t 0.12666666666666665\n",
      "Loss:\t 183.53533732230693\n",
      "Fraction wrong:\t 0.12666666666666665\n",
      "Loss:\t 183.29827465906882\n",
      "Fraction wrong:\t 0.12666666666666665\n",
      "Loss:\t 183.07098065159863\n",
      "Fraction wrong:\t 0.13\n",
      "Loss:\t 182.8529776681721\n",
      "Fraction wrong:\t 0.13\n",
      "Loss:\t 182.64381284189074\n",
      "Fraction wrong:\t 0.13\n",
      "Loss:\t 182.4430567554646\n",
      "Fraction wrong:\t 0.13\n",
      "Loss:\t 182.2503021965154\n",
      "Fraction wrong:\t 0.13\n",
      "Loss:\t 182.0651629795957\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 181.88727283133207\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 181.71628433529418\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 181.55186793337842\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 181.39371098066056\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 181.24151685085045\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 181.09500408961603\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 180.95390561321526\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 180.81796794999096\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 180.68695052242964\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 180.56062496760097\n",
      "Fraction wrong:\t 0.13333333333333336\n",
      "Loss:\t 180.43877449391957\n",
      "Fraction wrong:\t 0.13333333333333336\n"
     ]
    }
   ],
   "source": [
    "w = Vec(A.D[1], {k: 0 for k in A.D[1]}) # try again with 0's hypothesis\n",
    "w = gradient_descent(A, b, w, sigma, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:\t 180.35992492345744\n",
      "Final fraction wrong:\t 0.13333333333333336\n"
     ]
    }
   ],
   "source": [
    "print('Final loss:\\t', loss(A, b, w))\n",
    "print('Final fraction wrong:\\t', fraction_wrong(A, b, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is _dramatically better_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holding onto these nice results:\n",
    "best_w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\t 790140453.9175893\n",
      "Fraction wrong:\t 0.7692307692307692\n",
      "Loss:\t 122759.50090194705\n",
      "Fraction wrong:\t 0.676923076923077\n",
      "Loss:\t 120317.9601365443\n",
      "Fraction wrong:\t 0.6730769230769231\n",
      "Loss:\t 118156.79449082051\n",
      "Fraction wrong:\t 0.6653846153846154\n",
      "Loss:\t 116211.02127333778\n",
      "Fraction wrong:\t 0.6615384615384615\n",
      "Loss:\t 114431.5759626456\n",
      "Fraction wrong:\t 0.6576923076923077\n",
      "Loss:\t 112781.40121804012\n",
      "Fraction wrong:\t 0.6576923076923077\n",
      "Loss:\t 111232.49695748888\n",
      "Fraction wrong:\t 0.6461538461538462\n",
      "Loss:\t 109763.6953322771\n",
      "Fraction wrong:\t 0.6423076923076922\n",
      "Loss:\t 108358.98246396436\n",
      "Fraction wrong:\t 0.6346153846153846\n",
      "Loss:\t 107006.23258360621\n",
      "Fraction wrong:\t 0.6307692307692307\n",
      "Loss:\t 105696.25323056377\n",
      "Fraction wrong:\t 0.6269230769230769\n",
      "Loss:\t 104422.0650719458\n",
      "Fraction wrong:\t 0.6269230769230769\n",
      "Loss:\t 103178.35868768365\n",
      "Fraction wrong:\t 0.6269230769230769\n",
      "Loss:\t 101961.08483424848\n",
      "Fraction wrong:\t 0.6269230769230769\n",
      "Loss:\t 100767.14538642662\n",
      "Fraction wrong:\t 0.6230769230769231\n",
      "Loss:\t 99594.16021690317\n",
      "Fraction wrong:\t 0.6230769230769231\n",
      "Loss:\t 98440.29135302918\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 97304.11033575721\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 96184.49816449691\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 95080.56982045672\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 93991.61732875563\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 92917.06680378439\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 91856.44604174467\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 90809.3600686776\n",
      "Fraction wrong:\t 0.6153846153846154\n",
      "Loss:\t 89775.47268915469\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 88754.49256118406\n",
      "Fraction wrong:\t 0.6192307692307693\n",
      "Loss:\t 87746.16268520724\n",
      "Fraction wrong:\t 0.6153846153846154\n",
      "Loss:\t 86750.25246834826\n",
      "Fraction wrong:\t 0.6153846153846154\n",
      "Loss:\t 85766.55173121268\n",
      "Fraction wrong:\t 0.6153846153846154\n",
      "Loss:\t 84794.8661800077\n",
      "Fraction wrong:\t 0.6153846153846154\n",
      "Loss:\t 83835.01398402487\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 82886.82318698172\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 81950.12974743376\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 81024.776053791\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 80110.60979742711\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 79207.48311600092\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 78315.25194070308\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 77433.77549742276\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 76562.91592412513\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 75702.53797598393\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 74852.50879681228\n",
      "Fraction wrong:\t 0.6115384615384616\n",
      "Loss:\t 74012.69774060087\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 73182.97623095241\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 72363.2176491971\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 71553.29724424037\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 70753.09205889475\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 69962.48086874063\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 69181.34413052924\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 68409.56393786948\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 67647.02398250122\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 66893.60951986581\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 66149.20733800314\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 65413.70572904354\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 64686.994462734554\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 63968.96476158667\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 63259.50927731721\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 62558.52206835035\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 61865.898578190405\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 61181.5356145286\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 60505.331328975386\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 59837.185197336185\n",
      "Fraction wrong:\t 0.6038461538461538\n",
      "Loss:\t 59176.99800036952\n",
      "Fraction wrong:\t 0.6\n",
      "Loss:\t 58524.67180497547\n",
      "Fraction wrong:\t 0.6\n",
      "Loss:\t 57880.1099457784\n",
      "Fraction wrong:\t 0.6\n",
      "Loss:\t 57243.217007074054\n",
      "Fraction wrong:\t 0.6\n",
      "Loss:\t 56613.898805115015\n",
      "Fraction wrong:\t 0.6\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "w = Vec(A.D[1], {k: randint(0, 1) for k in A.D[1]})\n",
    "w = gradient_descent(A, b, w, sigma, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:\t 198424.73800185614\n",
      "Final fraction wrong:\t 0.6233333333333333\n"
     ]
    }
   ],
   "source": [
    "print('Final loss:\\t', loss(A, b, w))\n",
    "print('Final fraction wrong:\\t', fraction_wrong(A, b, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.13:** After you have used your gradient descent code to find a hypothesis vector $\\boldsymbol{w}$, see how well this hypothesis works for the data in the file `validate.data`. What is the percentage of samples that are incorrectly classified? Is it greater or smaller than the success rate on the training data? Can you explain the difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_A, validation_b = read_training_data('validate.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:\t 152.62023375699934\n",
      "Validation fraction wrong:\t 0.04999999999999999\n"
     ]
    }
   ],
   "source": [
    "print('Validation loss:\\t', loss(validation_A, validation_b, best_w))\n",
    "print('Validation fraction wrong:\\t', fraction_wrong(validation_A, validation_b, best_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am surprised to see that the validation error is actually _lower_ than the training error!  Normally this is not the case.  This tells us two things: we definitely did not overfit the data, _and_ we got a little lucky :)\n",
    "\n",
    "Normally, learning a function that fits one set very will will not generalize to new data.  I think what this tells us at a higher level is that the function might be a relatively simple one to learn (the positive examples are easier to discern from the feature data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:\t 56198.51556405085\n",
      "Validation fraction wrong:\t 0.6\n"
     ]
    }
   ],
   "source": [
    "# Trying with the hypothesis learned from our last trial (random starting hypothesis)\n",
    "print('Validation loss:\\t', loss(validation_A, validation_b, w))\n",
    "print('Validation fraction wrong:\\t', fraction_wrong(validation_A, validation_b, w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an inner product for vectors over $\\mathbb{R}$**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One inner product vor vectors over $\\mathbb{R}$ is the dot-product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is norm defined in terms of dot-product?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norm is defined as the square-root of the dot-product of a vector with itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does it mean for two vectors to be orthogonal?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two vectors to be orthogonal, it means their inner product is 0.  Geometrically, orthogonality between 2-vectors over $\\mathbb{R}$ means that they are perpendicular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the Pythagorean Theorem for vectors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pythagorean Theorem for vectors states that for orthogonal vectors, the square of the norm of the sum of the vectors is equal to the sum of the squares of the norms of the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is parallel-perpendicular decomposition of a vector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every pair of vectors $\\boldsymbol{v}$ and $\\boldsymbol{a}$, $\\boldsymbol{v}$ can be decomposed into a component that is _parallel with_ $\\boldsymbol{a}$ (meaning the closest projection of $\\boldsymbol{v}$ onto the line spanned by $\\boldsymbol{a}$), and a component that is _perpendicular to_ $\\boldsymbol{a}$ (meaning a vector in the direction perpendicular to the line spanned by $\\boldsymbol{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does one find the projection of a vector $\\boldsymbol{b}$ orthogonal to another vector $\\boldsymbol{v}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can find such a vector with the equation \n",
    "\n",
    "$\\boldsymbol{v} \\cdot \\frac{\\langle\\boldsymbol{b},\\boldsymbol{v}\\rangle}{\\langle\\boldsymbol{v},\\boldsymbol{v}\\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can linear algebra help in optimizing a nonlinear function?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra can be used in computing the _gradient_ of a nonlinear function at a given point, where the gradient is found by taking the derivative of the nonlinear function at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8.6.1:** For each of the following problems, compute the norm of given vector $\\boldsymbol{v}$:\n",
    "\n",
    "**a)** $\\boldsymbol{v} = [2,2,1]$\n",
    "\n",
    "$\\|\\boldsymbol{v}\\| = \\sqrt{9} = 3$\n",
    "\n",
    "**b)** $\\boldsymbol{v} = [\\sqrt{2},\\sqrt{3},\\sqrt{5},\\sqrt{6}]$\n",
    "\n",
    "$\\|\\boldsymbol{v}\\| = \\sqrt{16} = 4$\n",
    "\n",
    "**c)** $\\boldsymbol{v} = [1,1,1,1,1,1,1,1,1]$\n",
    "\n",
    "$\\|\\boldsymbol{v}\\| = \\sqrt{9} = 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8.6.2:** For each of the following $\\boldsymbol{a}$, $\\boldsymbol{b}$, find the vector in $\\text{Span}\\space\\{\\boldsymbol{a}\\}$ that is closest to $\\boldsymbol{b}$:\n",
    "\n",
    "1. $\\boldsymbol{a} = [1,2]$, $\\boldsymbol{b} = [2,3]$\n",
    "\n",
    "  $\\boldsymbol{a} \\cdot \\frac{\\langle\\boldsymbol{b},\\boldsymbol{a}\\rangle}{\\langle\\boldsymbol{a},\\boldsymbol{a}\\rangle} = [1,2] \\cdot \\frac{8}{5} = [\\frac{8}{5}, \\frac{16}{5}]$\n",
    "\n",
    "2. $\\boldsymbol{a} = [0,1,0]$, $\\boldsymbol{b} = [1.414,1,1.732]$\n",
    "\n",
    "  (Since the norm of $\\boldsymbol{v}$ is 1), $\\boldsymbol{a} \\cdot \\langle\\boldsymbol{b},\\boldsymbol{a}\\rangle = [0,1,0] \\cdot 1 = [0,1,0]$\n",
    "\n",
    "3. $\\boldsymbol{a} = [-3,-2,-1,4]$, $\\boldsymbol{b} = [7,2,5,0]$\n",
    "\n",
    "  $\\boldsymbol{a} \\cdot \\langle\\boldsymbol{b},\\boldsymbol{a}\\rangle = [-3,-2,-1,4] \\cdot \\frac{-30}{18} = [-3,-2,-1,4] \\cdot \\frac{-5}{3} = [5,\\frac{10}{3},\\frac{5}{3},\\frac{-20}{3}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8.6.3:** For each of the following $\\boldsymbol{a}$, $\\boldsymbol{b}$, find $\\boldsymbol{b}^{\\perp\\boldsymbol{a}}$ and $\\boldsymbol{b}^{\\|\\boldsymbol{a}}$\n",
    "\n",
    "1. $\\boldsymbol{a} = [3,0], \\boldsymbol{b} = [2,1]$\n",
    "\n",
    "  $\\boldsymbol{b}^{\\|\\boldsymbol{a}} = \\boldsymbol{a} \\cdot \\frac{\\langle\\boldsymbol{b},\\boldsymbol{a}\\rangle}{\\langle\\boldsymbol{a},\\boldsymbol{a}\\rangle} = [3,0] \\frac{6}{9} = [2,0]$  \n",
    "  $\\boldsymbol{b}^{\\perp\\boldsymbol{a}} = \\boldsymbol{b} - \\boldsymbol{b}^{\\|\\boldsymbol{a}} = [2,1] - [2,0] = [0,1]$\n",
    "  \n",
    "  (Of course, this could be found much more simply with geometrical reasoning since $\\boldsymbol{a}$ lies along the $x$-axis.)\n",
    "\n",
    "1. $\\boldsymbol{a} = [1,2,-1], \\boldsymbol{b} = [1,1,4]$\n",
    "\n",
    "  $\\boldsymbol{b}^{\\|\\boldsymbol{a}} = \\boldsymbol{a} \\cdot \\frac{\\langle\\boldsymbol{b},\\boldsymbol{a}\\rangle}{\\langle\\boldsymbol{a},\\boldsymbol{a}\\rangle} = [1,2,-1] \\frac{-1}{2} = [\\frac{-1}{2},-1,\\frac{1}{2}]$  \n",
    "  $\\boldsymbol{b}^{\\perp\\boldsymbol{a}} = \\boldsymbol{b} - \\boldsymbol{b}^{\\|\\boldsymbol{a}} = [1,1,4] - [\\frac{-1}{2},-1,\\frac{1}{2}] = [\\frac{3}{2},2,\\frac{7}{2}]$\n",
    "\n",
    "1. $\\boldsymbol{a} = [3,3,12], \\boldsymbol{b} = [1,1,4]$\n",
    "\n",
    "  $\\boldsymbol{b}^{\\|\\boldsymbol{a}} = \\boldsymbol{a} \\cdot \\frac{\\langle\\boldsymbol{b},\\boldsymbol{a}\\rangle}{\\langle\\boldsymbol{a},\\boldsymbol{a}\\rangle} = [3,3,12] \\frac{54}{162} = [3,3,12] \\frac{1}{3} = [1,1,4]$  \n",
    "  $\\boldsymbol{b}^{\\perp\\boldsymbol{a}} = \\boldsymbol{b} - \\boldsymbol{b}^{\\|\\boldsymbol{a}} = [1,1,4] - [1,1,4] = 0$\n",
    "  \n",
    "  (Note that this also could have been done quickly be seeing that $\\boldsymbol{b}$ is a scalar multiple of $\\boldsymbol{a}$, and thus lies on the line composed of its span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
